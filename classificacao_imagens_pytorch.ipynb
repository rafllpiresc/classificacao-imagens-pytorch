{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Classificação de Imagem"
      ],
      "metadata": {
        "id": "kSO134DVES9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNkwNuErM7Q4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms, ToTensor, Resize, Compose\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yod9JOZP_vgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013e22c5-66ef-4c0a-b453-e0e982e6094e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Verificando a disponibilidade de CPU ou GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6GtPVAWUwhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67db66f3-5499-4025-e448-e0432988f8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/DL_02_2024/MEU/Class_Imagens\n"
          ]
        }
      ],
      "source": [
        "# Monta o Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Muda o diretório para a pasta especificada no Google Drive\n",
        "%cd /content/drive/MyDrive/DL_02_2024/MEU/Class_Imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Processamento"
      ],
      "metadata": {
        "id": "HoFj_ixmEbvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF9xxE4XYy9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949bfbc1-add1-4122-ba2e-1d1e452ac32a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted 0 images\n"
          ]
        }
      ],
      "source": [
        "num_skipped = 0  # Inicializa o contador de imagens corrompidas\n",
        "\n",
        "for dataset in (\"train\", \"test\"):  # Itera sobre os conjuntos de dados de treino e teste\n",
        "    dataset_path = os.path.join(\"Cars Dataset\", dataset)  # Cria o caminho para o conjunto de dados atual\n",
        "\n",
        "    for brand_folder in os.listdir(dataset_path):  # Itera sobre as pastas de marcas dentro do conjunto de dados\n",
        "        folder_path = os.path.join(dataset_path, brand_folder)  # Cria o caminho para a pasta da marca atual\n",
        "\n",
        "        for fname in os.listdir(folder_path):  # Itera sobre os arquivos dentro da pasta da marca\n",
        "            fpath = os.path.join(folder_path, fname)  # Cria o caminho completo para o arquivo atual\n",
        "\n",
        "            try:\n",
        "                fobj = open(fpath, \"rb\")  # Abre o arquivo em modo binário de leitura\n",
        "                is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)  # Verifica se o arquivo contém o marcador \"JFIF\" nos primeiros 10 bytes\n",
        "\n",
        "            finally:\n",
        "                fobj.close()  # Fecha o arquivo\n",
        "\n",
        "            if not is_jfif:  # Se o arquivo não for um JFIF válido\n",
        "                num_skipped += 1  # Incrementa o contador de imagens corrompidas\n",
        "                os.remove(fpath)  # Remove o arquivo corrompido\n",
        "\n",
        "print(\"Deleted %d images\" % num_skipped)  # Imprime o número total de imagens deletadas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento em Modelo Simples"
      ],
      "metadata": {
        "id": "qBE8GQCFEffI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZBX2u0GdCX5"
      },
      "outputs": [],
      "source": [
        "# Defina o diretório raiz das pastas de treinamento e teste\n",
        "root_dir = 'Cars Dataset'\n",
        "\n",
        "# Nome das pastas de treinamento e teste\n",
        "train_folder_name = 'train'\n",
        "test_folder_name = 'test'\n",
        "\n",
        "# Caminhos para as pastas de treinamento e teste\n",
        "train_folder_path = os.path.join(root_dir, train_folder_name)\n",
        "test_folder_path = os.path.join(root_dir, test_folder_name)\n",
        "\n",
        "# Criando as pastas de treinamento e teste, se ainda não existirem\n",
        "os.makedirs(train_folder_path, exist_ok=True)\n",
        "os.makedirs(test_folder_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw22HjVDiE_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd8f8a9-a47d-427b-b956-501c2a012818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 3348\n",
            "Number of testing samples: 812\n",
            "Classes: ['Audi', 'Hyundai Creta', 'Mahindra Scorpio', 'Rolls Royce', 'Swift', 'Tata Safari', 'Toyota Innova']\n"
          ]
        }
      ],
      "source": [
        "# Caminhos para as pastas de treinamento e teste\n",
        "train_folder = 'Cars Dataset/train'\n",
        "test_folder = 'Cars Dataset/test'\n",
        "\n",
        "# Primeiro, as imagens são convertidas para tensores (ToTensor), e depois normalizadas com média e desvio padrão de 0.5 para cada canal de cor (RGB).\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# As transformações definidas anteriormente são aplicadas às imagens.\n",
        "train_dataset = ImageFolder(train_folder, transform=transform)\n",
        "test_dataset = ImageFolder(test_folder, transform=transform)\n",
        "\n",
        "# Criação dos data loaders de treinamento e teste\n",
        "batch_size = 32  # Define o tamanho do lote\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Verificação da estrutura do dataset\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "print(f\"Classes: {train_dataset.classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJNrwOfeW8Ni"
      },
      "outputs": [],
      "source": [
        "# Número de Épocas de treinamento\n",
        "epocas = 10\n",
        "\n",
        "# Criar uma pasta para salvar os modelos\n",
        "folder_path = 'Modelos'\n",
        "os.makedirs(folder_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad1m7V_EkxHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394cd595-2775-40fa-fa5a-826a8e4cba4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 3348\n",
            "Number of testing samples: 812\n",
            "Classes: ['Audi', 'Hyundai Creta', 'Mahindra Scorpio', 'Rolls Royce', 'Swift', 'Tata Safari', 'Toyota Innova']\n",
            "Epoch 1, Loss: 1.8390102113996234\n",
            "Epoch 2, Loss: 1.628180301757086\n",
            "Epoch 3, Loss: 1.2572632568223137\n",
            "Epoch 4, Loss: 0.8889616248153505\n",
            "Epoch 5, Loss: 0.5428347059658596\n",
            "Epoch 6, Loss: 0.31636905343759625\n",
            "Epoch 7, Loss: 0.1924787987910566\n",
            "Epoch 8, Loss: 0.07612586427657377\n",
            "Epoch 9, Loss: 0.11907439876702569\n",
            "Epoch 10, Loss: 0.14057208705870877\n",
            "Perda de entropia cruzada no conjunto de teste: 1.9936613417588747\n"
          ]
        }
      ],
      "source": [
        "# Caminhos para as pastas de treinamento e teste\n",
        "train_folder = 'Cars Dataset/train'\n",
        "test_folder = 'Cars Dataset/test'\n",
        "\n",
        "# Define as transformações a serem aplicadas às imagens\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),  # Redimensiona todas as imagens para 224x224\n",
        "    ToTensor()           # Converte a imagem para um tensor PyTorch\n",
        "])\n",
        "\n",
        "# Cria datasets para treinamento e teste\n",
        "train_dataset = ImageFolder(train_folder, transform=transform)\n",
        "test_dataset = ImageFolder(test_folder, transform=transform)\n",
        "\n",
        "# Cria data loaders para treinamento e teste\n",
        "batch_size = 32  # Define o tamanho do lote\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Verifica a estrutura do dataset\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Define o modelo\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)  # Primeira camada convolucional\n",
        "        self.pool = nn.MaxPool2d(2, 2)   # Camada de pooling\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # Segunda camada convolucional\n",
        "        self.fc1 = nn.Linear(16 * 53 * 53, 120)  # Primeira camada totalmente conectada\n",
        "        self.fc2 = nn.Linear(120, 84)    # Segunda camada totalmente conectada\n",
        "        self.fc3 = nn.Linear(84, num_classes)      # Terceira camada totalmente conectada\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Passa pela primeira camada convolucional e pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Passa pela segunda camada convolucional e pooling\n",
        "        x = x.view(-1, 16 * 53 * 53)          # Achata o tensor para a camada totalmente conectada\n",
        "        x = F.relu(self.fc1(x))               # Passa pela primeira camada totalmente conectada\n",
        "        x = F.relu(self.fc2(x))               # Passa pela segunda camada totalmente conectada\n",
        "        x = self.fc3(x)                       # Passa pela terceira camada totalmente conectada\n",
        "        return x\n",
        "\n",
        "model = SimpleModel(num_classes).to(device)\n",
        "\n",
        "# Função de perda\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Otimizador\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Função de treinamento (exemplo simplificado)\n",
        "def treinamento():\n",
        "    model.train()  # Coloca o modelo em modo de treinamento\n",
        "    for epoch in range(epocas):  # Número de épocas\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Zera os gradientes do otimizador\n",
        "            outputs = model(images)  # Passa as imagens pelo modelo\n",
        "            loss = criterion(outputs, labels)  # Calcula a perda\n",
        "            loss.backward()  # Calcula os gradientes\n",
        "            optimizer.step()  # Atualiza os pesos do modelo\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Treinamento do modelo\n",
        "treinamento()\n",
        "\n",
        "# Salvar o modelo na pasta criada\n",
        "folder_path = 'Cars Dataset/models'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "file_path = os.path.join(folder_path, 'modelosimples1.pth')\n",
        "torch.save(model.state_dict(), file_path)\n",
        "\n",
        "# Função para calcular a perda de entropia cruzada no conjunto de teste\n",
        "def calcular_perda_teste():\n",
        "    model.eval()  # Coloca o modelo em modo de avaliação\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():  # Desabilita a computação do gradiente\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)  # Passa as imagens pelo modelo\n",
        "            loss = criterion(outputs, labels)  # Calcula a perda\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    print(f'Perda de entropia cruzada no conjunto de teste: {avg_test_loss}')\n",
        "\n",
        "# Calcule a perda de entropia cruzada no conjunto de teste\n",
        "calcular_perda_teste()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento em Modelo Complexo"
      ],
      "metadata": {
        "id": "N6_L2OgwElb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGWhkZcl_lgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92b020d-ce7f-4548-bb2b-532b7feb03b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.5297067119961694\n",
            "Epoch 2, Loss: 0.8231904495330084\n",
            "Epoch 3, Loss: 0.33314117519628433\n",
            "Epoch 4, Loss: 0.09618625945988156\n",
            "Epoch 5, Loss: 0.03268284432235218\n",
            "Epoch 6, Loss: 0.016293467000304235\n",
            "Epoch 7, Loss: 0.010897889535962825\n",
            "Epoch 8, Loss: 0.00813590305458222\n",
            "Epoch 9, Loss: 0.00654165248519608\n",
            "Epoch 10, Loss: 0.005314404306755889\n",
            "Perda de entropia cruzada no conjunto de teste: 1.1393861805017178\n"
          ]
        }
      ],
      "source": [
        "class DeepModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 5)  # Primeira camada convolucional (entrada: 3 canais, saída: 16 canais, kernel: 5x5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Camada de pooling (reduz a dimensão pela metade)\n",
        "        self.bn1 = nn.BatchNorm2d(16)  # Normalização em lote para a primeira camada convolucional\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)  # Segunda camada convolucional (entrada: 16 canais, saída: 32 canais, kernel: 5x5)\n",
        "        self.bn2 = nn.BatchNorm2d(32)  # Normalização em lote para a segunda camada convolucional\n",
        "\n",
        "        # Camadas totalmente conectadas (fully connected)\n",
        "        self.fc1 = nn.Linear(32 * 53 * 53, 120)  # Primeira camada totalmente conectada\n",
        "        self.fc2 = nn.Linear(120, 84)  # Segunda camada totalmente conectada\n",
        "        self.fc3 = nn.Linear(84, 7)  # Terceira camada totalmente conectada (saída: 7 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Aplicação da primeira camada convolucional, seguida de ReLU e pooling\n",
        "        x = self.bn1(x)  # Aplicação da normalização em lote\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Aplicação da segunda camada convolucional, seguida de ReLU e pooling\n",
        "        x = self.bn2(x)  # Aplicação da normalização em lote\n",
        "        x = torch.flatten(x, 1)  # Achatar o tensor para uma dimensão\n",
        "        x = F.relu(self.fc1(x))  # Aplicação da primeira camada totalmente conectada com ReLU\n",
        "        x = F.relu(self.fc2(x))  # Aplicação da segunda camada totalmente conectada com ReLU\n",
        "        x = self.fc3(x)  # Aplicação da terceira camada totalmente conectada (saída)\n",
        "        return x  # Retorna a saída final\n",
        "\n",
        "model = DeepModel().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Treinamento do modelo\n",
        "def treinamento():\n",
        "    model.train()  # Coloca o modelo em modo de treinamento\n",
        "\n",
        "    for epoch in range(epocas):  # Loop sobre o número de épocas\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:  # Loop sobre o conjunto de dados de treinamento\n",
        "            images = images.to(device)  # Move as imagens para o dispositivo (GPU ou CPU)\n",
        "            labels = labels.to(device)  # Move os rótulos para o dispositivo\n",
        "\n",
        "            optimizer.zero_grad()  # Zera os gradientes do otimizador\n",
        "            outputs = model(images)  # Passa as imagens pelo modelo para obter as previsões\n",
        "            loss = criterion(outputs, labels)  # Calcula a perda (loss) entre as previsões e os rótulos\n",
        "            loss.backward()  # Calcula os gradientes da perda\n",
        "            optimizer.step()  # Atualiza os pesos do modelo\n",
        "\n",
        "            running_loss += loss.item()  # Acumula a perda para a época atual\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")  # Imprime a perda média por época\n",
        "\n",
        "treinamento()\n",
        "\n",
        "# Função para calcular a perda de entropia cruzada no conjunto de teste\n",
        "def calcular_perda_teste():\n",
        "    model.eval()  # Coloca o modelo em modo de avaliação\n",
        "    test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Desabilita a computação do gradiente\n",
        "        for images, labels in test_loader:  # Loop sobre o conjunto de dados de teste\n",
        "            images = images.to(device)  # Move as imagens para o dispositivo\n",
        "            labels = labels.to(device)  # Move os rótulos para o dispositivo\n",
        "            outputs = model(images)  # Passa as imagens pelo modelo para obter as previsões\n",
        "            loss = criterion(outputs, labels)  # Calcula a perda (loss) entre as previsões e os rótulos\n",
        "            test_loss += loss.item()  # Acumula a perda para o conjunto de teste\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)  # Calcula a perda média no conjunto de teste\n",
        "    print(f'Perda de entropia cruzada no conjunto de teste: {avg_test_loss}')  # Imprime a perda média no conjunto de teste\n",
        "\n",
        "# Calcule a perda de entropia cruzada no conjunto de teste\n",
        "calcular_perda_teste()\n",
        "\n",
        "# Salvar o modelo na pasta criada\n",
        "folder_path = 'Cars Dataset/models'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "file_path = os.path.join(folder_path, 'modeloDeepModel2.pth')\n",
        "torch.save(model.state_dict(), file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento em Modelo Complexo com múltiplos otimizadores"
      ],
      "metadata": {
        "id": "GdftGcN6EoWy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTqyja3Y_ldA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb80d50-598c-4786-c665-7bad432fa225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando com function\n",
            "Epoch 1, Loss: 1.7629346336637224\n",
            "Epoch 2, Loss: 0.7161129390909559\n",
            "Epoch 3, Loss: 0.20866118165708725\n",
            "Epoch 4, Loss: 0.05241065383667037\n",
            "Epoch 5, Loss: 0.01628165067328761\n",
            "Epoch 6, Loss: 0.006454176151947606\n",
            "Epoch 7, Loss: 0.0037361174677720383\n",
            "Epoch 8, Loss: 0.004836687911301851\n",
            "Epoch 9, Loss: 0.00536273560553257\n",
            "Epoch 10, Loss: 0.42209279685990797\n",
            "Perda de entropia cruzada no conjunto de teste: 2.898401750968053\n",
            "\n",
            "Treinando com function\n",
            "Epoch 1, Loss: 1.5381419215883527\n",
            "Epoch 2, Loss: 0.8507307512419564\n",
            "Epoch 3, Loss: 0.36886962382566363\n",
            "Epoch 4, Loss: 0.1239974129767645\n",
            "Epoch 5, Loss: 0.04000207536986896\n",
            "Epoch 6, Loss: 0.018969126369449356\n",
            "Epoch 7, Loss: 0.01249837643866028\n",
            "Epoch 8, Loss: 0.009319849483047923\n",
            "Epoch 9, Loss: 0.0070883393287658695\n",
            "Epoch 10, Loss: 0.005770426318936405\n",
            "Perda de entropia cruzada no conjunto de teste: 1.1484375366797814\n",
            "\n",
            "Treinando com function\n",
            "Epoch 1, Loss: 3.2340169656844364\n",
            "Epoch 2, Loss: 1.180960364001138\n",
            "Epoch 3, Loss: 0.7100994413807279\n",
            "Epoch 4, Loss: 0.3998326119922456\n",
            "Epoch 5, Loss: 0.22266079815370696\n",
            "Epoch 6, Loss: 0.16325413199762503\n",
            "Epoch 7, Loss: 0.0767835887503766\n",
            "Epoch 8, Loss: 0.09715692928681771\n",
            "Epoch 9, Loss: 0.06800403069660423\n",
            "Epoch 10, Loss: 0.05113799076172568\n",
            "Perda de entropia cruzada no conjunto de teste: 8.562368489228762\n"
          ]
        }
      ],
      "source": [
        "model = DeepModel().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Função para treinamento\n",
        "def treinamento(optimizer, epochs):\n",
        "    model.train()  # Coloca o modelo em modo de treinamento\n",
        "    for epoch in range(epochs):  # Loop sobre o número de épocas\n",
        "        running_loss = 0.0  # Inicializa a perda acumulada para a época\n",
        "        for images, labels in train_loader:  # Loop sobre os lotes de dados de treinamento\n",
        "            images = images.to(device)  # Move as imagens para o dispositivo (CPU ou GPU)\n",
        "            labels = labels.to(device)  # Move os rótulos para o dispositivo\n",
        "\n",
        "            optimizer.zero_grad()  # Zera os gradientes do otimizador\n",
        "            outputs = model(images)  # Passa as imagens pelo modelo para obter as previsões\n",
        "            loss = criterion(outputs, labels)  # Calcula a perda entre as previsões e os rótulos verdadeiros\n",
        "            loss.backward()  # Calcula os gradientes da perda em relação aos parâmetros do modelo\n",
        "            optimizer.step()  # Atualiza os parâmetros do modelo com base nos gradientes\n",
        "\n",
        "            running_loss += loss.item()  # Acumula a perda do lote atual\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")  # Imprime a perda média da época\n",
        "\n",
        "# Função para calcular a perda de teste\n",
        "def calcular_perda_teste():\n",
        "    model.eval()  # Coloca o modelo em modo de avaliação\n",
        "    test_loss = 0.0  # Inicializa a perda acumulada para o conjunto de teste\n",
        "    with torch.no_grad():  # Desativa o cálculo de gradientes\n",
        "        for images, labels in test_loader:  # Loop sobre os lotes de dados de teste\n",
        "            images = images.to(device)  # Move as imagens para o dispositivo\n",
        "            labels = labels.to(device)  # Move os rótulos para o dispositivo\n",
        "            outputs = model(images)  # Passa as imagens pelo modelo para obter as previsões\n",
        "            loss = criterion(outputs, labels)  # Calcula a perda entre as previsões e os rótulos verdadeiros\n",
        "            test_loss += loss.item()  # Acumula a perda do lote atual\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)  # Calcula a perda média no conjunto de teste\n",
        "    print(f'Perda de entropia cruzada no conjunto de teste: {avg_test_loss}')  # Imprime a perda média do conjunto de teste\n",
        "\n",
        "# Função para testar diferentes otimizadores\n",
        "def testar_otimizadores(optimizers, epochs):\n",
        "    for opt in optimizers:  # Loop sobre a lista de otimizadores\n",
        "        print(f\"\\nTreinando com {opt.__class__.__name__}\")  # Imprime o nome da classe do otimizador atual\n",
        "        # Reinicializa os parâmetros do modelo\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "        optimizer = opt(model.parameters())  # Inicializa o otimizador com os parâmetros do modelo\n",
        "        treinamento(optimizer, epochs)  # Treina o modelo com o otimizador atual\n",
        "        calcular_perda_teste()  # Calcula e imprime a perda no conjunto de teste\n",
        "\n",
        "# Lista de otimizadores a serem testados\n",
        "optimizers = [\n",
        "    lambda params: optim.Adam(params, lr=0.001, weight_decay=0.001),  # Otimizador Adam com taxa de aprendizado e decaimento de peso específicos\n",
        "    lambda params: optim.SGD(params, lr=0.001, momentum=0.9),  # Otimizador SGD com taxa de aprendizado e momento específicos\n",
        "    lambda params: optim.RMSprop(params, lr=0.001),  # Otimizador RMSprop com taxa de aprendizado específica\n",
        "]\n",
        "\n",
        "# Testar os otimizadores\n",
        "testar_otimizadores(optimizers, epocas)  # Chama a função para testar os otimizadores com o número de épocas especificado"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Teste"
      ],
      "metadata": {
        "id": "oTYWFyOTEt1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYyHdJXyNmmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c390c33-690c-45b1-8d04-154252e9af7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-b34e0802f2cc>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A imagem foi classificada como: Audi\n"
          ]
        }
      ],
      "source": [
        "# Pasta onde o modelo foi salvo\n",
        "model_path = 'Cars Dataset/models/modeloDeepModel2.pth'\n",
        "\n",
        "# Carrega o modelo\n",
        "model = DeepModel().to(device)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# Define as transformações para a imagem de entrada\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Caminho da imagem que você deseja classificar\n",
        "image_path = 'Cars Dataset/test/Audi/1000.jpg'\n",
        "\n",
        "# Carrega a imagem\n",
        "image = Image.open(image_path)\n",
        "image = transform(image).unsqueeze(0).to(device)  # Adiciona uma dimensão de batch\n",
        "\n",
        "# Faz a previsão\n",
        "with torch.no_grad():\n",
        "  output = model(image)\n",
        "  _, predicted = torch.max(output, 1)\n",
        "\n",
        "# Imprime a previsão\n",
        "class_names = train_dataset.classes\n",
        "predicted_class = class_names[predicted.item()]\n",
        "print(f\"A imagem foi classificada como: {predicted_class}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}